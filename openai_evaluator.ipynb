{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. --- IMPORTS AND CONFIGURATION ---",
   "id": "470bace0b6888015"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T02:32:50.303433Z",
     "start_time": "2025-07-29T02:32:49.753961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Importing the custom preprocessing function for BASSE dataset and preparation tasks\n",
    "from preprocessing.basse_preprocessing import process_basse_summaries\n",
    "from preparation.basse_preparation import prepare_basse_tasks\n",
    "\n",
    "# Importing the custom preprocessing functions for FLARES dataset and preparation tasks\n",
    "from preprocessing.flares_preprocessing import load_and_merge_datasets, process_and_flatten_data\n",
    "from preparation.flares_preparation import prepare_flares_tasks\n",
    "\n",
    "# Define the Pydantic models for structured output\n",
    "from pydantic_models.basse_pydantic_models import DetailedEvaluation\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "# Load environment variables (will look for .env file)\n",
    "load_dotenv()\n",
    "\n",
    "# --- Dynamic AI Provider Configuration ---\n",
    "\n",
    "# Read environment variables\n",
    "ai_provider = os.getenv(\"MODEL_PROVIDER\", \"openai\").lower() # Default to 'openai'\n",
    "model_name = os.getenv(\"MODEL\")\n",
    "\n",
    "api_key = None\n",
    "base_url = None\n",
    "\n",
    "print(f\"Using provider: {ai_provider.capitalize()}\")\n",
    "\n",
    "if ai_provider == \"openai\":\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    # base_url is not needed; the client defaults to OpenAI's endpoint.\n",
    "elif ai_provider == \"gemini\":\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "elif ai_provider == \"anthropic\":\n",
    "    api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "    base_url = \"https://api.anthropic.com/v1/\"\n",
    "else:\n",
    "    print(f\"Error: AI provider '{ai_provider}' is not supported. Options: openai, gemini, anthropic.\")\n",
    "    exit()\n",
    "\n",
    "# --- Client Initialization ---\n",
    "\n",
    "# Validate that the API key and model are defined\n",
    "if not api_key:\n",
    "    print(f\"Error: API key environment variable for '{ai_provider}' is not set.\")\n",
    "    exit()\n",
    "if not model_name:\n",
    "    print(\"Error: 'MODEL' environment variable is not set.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize the OpenAI client with the dynamic configuration\n",
    "try:\n",
    "    client = openai.OpenAI(api_key=api_key, base_url=base_url)\n",
    "except openai.OpenAIError as e:\n",
    "    print(f\"Error initializing the client for {ai_provider.capitalize()}.\")\n",
    "    print(e)\n",
    "    exit()\n",
    "\n",
    "# Now you can use the client and model name in your API calls\n",
    "print(f\"✅ Client initialized successfully. Model to use: {model_name}\")"
   ],
   "id": "9fb2a029de32d32b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provider: Anthropic\n",
      "✅ Client initialized successfully. Model to use: claude-3-5-haiku-latest\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. --- LOAD DATASETS ---",
   "id": "eebe3b8f481fa398"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T02:32:50.466359Z",
     "start_time": "2025-07-29T02:32:50.460717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_basse_dataset(basse_path: str, ):\n",
    "    \"\"\"\n",
    "    Loads the BASSE dataset from a JSONL file and processes it.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return process_basse_summaries(basse_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The BASSE dataset file was not found at path '{basse_path}'\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def load_flares_dataset(flares_path: list):\n",
    "    \"\"\"\n",
    "    Loads and processes the FLARES dataset from multiple JSON files.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load and merge the datasets\n",
    "        flares_datasets_merged = load_and_merge_datasets(flares_path)\n",
    "        # Process and flatten the data\n",
    "        return process_and_flatten_data(flares_datasets_merged)\n",
    "    except FileNotFoundError:\n",
    "        print(\n",
    "            f\"Error: The FLARES dataset files were not found at the specified paths: {flares_path[0]} and {flares_path[1]}\")\n",
    "        return []"
   ],
   "id": "50fa6f2717881f0",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. --- FUNCTION TO LOAD THE PROMPT ---",
   "id": "b9ba32a8ffff78dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T02:32:50.489365Z",
     "start_time": "2025-07-29T02:32:50.484361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_prompt_from_file(filepath: str) -> str:\n",
    "    \"\"\"Reads and returns the content of a text file.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The prompt file was not found at path '{filepath}'\")\n",
    "        return \"\"  # Return an empty string in case of an error"
   ],
   "id": "aa04cd91dbb0ba75",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. --- EVALUATION FUNCTION WITH STRUCTURED OUTPUT ---",
   "id": "850f8f3053ad599f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T02:32:50.511813Z",
     "start_time": "2025-07-29T02:32:50.504815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_extraction(document: str, extraction: str, prompt_template: str, pydantic_model: BaseModel) -> tuple:\n",
    "    \"\"\"\n",
    "    Prepares and calls the OpenAI API using 'response_model' for structured and validated output.\n",
    "    \"\"\"\n",
    "    if not prompt_template:\n",
    "        print(\"Cannot proceed without a prompt template.\")\n",
    "        return None, None\n",
    "\n",
    "    filled_prompt = prompt_template.format(\n",
    "        original_document=document,\n",
    "        extraction_to_evaluate=extraction\n",
    "    )\n",
    "\n",
    "    print(\"--- STARTING STRUCTURED EVALUATION ---\")\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": filled_prompt}],\n",
    "            # 1. We define the \"tool\" that the model should use.\n",
    "            #    We use .model_json_schema() to automatically generate the schema.\n",
    "            tools=[\n",
    "                {\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": \"save_evaluation\",\n",
    "                        \"description\": \"Saves the structured evaluation result of an extraction.\",\n",
    "                        \"parameters\": pydantic_model.model_json_schema()\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            # 2. We force the model to call our tool.\n",
    "            tool_choice={\"type\": \"function\", \"function\": {\"name\": \"save_evaluation\"}}\n",
    "        )\n",
    "\n",
    "        # 3. We extract the result from the tool call arguments.\n",
    "        #    The response is a JSON string, not an object.\n",
    "        tool_call = response.choices[0].message.tool_calls[0]\n",
    "        json_arguments = tool_call.function.arguments\n",
    "\n",
    "        # We capture tokens' usage\n",
    "        token_usage = response.usage\n",
    "\n",
    "        # 4. We parse and validate the JSON against our Pydantic model.\n",
    "        evaluation_object = pydantic_model.model_validate_json(json_arguments)\n",
    "\n",
    "        return evaluation_object, token_usage\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred calling the API or processing the response: {e}\")\n",
    "        return None, None"
   ],
   "id": "a04d9c9294cfe927",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. --- MAIN ITERATION AND STORAGE LOGIC ---",
   "id": "ceec2ddf7ee558d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T02:32:50.532143Z",
     "start_time": "2025-07-29T02:32:50.525202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_dataset(\n",
    "        dataset: list,\n",
    "        prepare_tasks_func: callable,\n",
    "        prompt_path: str,\n",
    "        pydantic_model: BaseModel\n",
    "):\n",
    "    \"\"\"\n",
    "    A generic main loop to process any dataset using a specific task preparer.\n",
    "    \"\"\"\n",
    "    prompt_template = load_prompt_from_file(prompt_path)\n",
    "    if not prompt_template:\n",
    "        return [], 0\n",
    "\n",
    "    all_evaluations = []\n",
    "    total_tokens = 0\n",
    "    env = os.environ.get(\"ENVIRONMENT\", \"development\")\n",
    "    docs_to_process = dataset[:5] if env == \"development\" else dataset\n",
    "\n",
    "    print(f\"Processing {len(docs_to_process)} documents...\")\n",
    "\n",
    "    for entry in docs_to_process:\n",
    "        # The preparer function handles the differences between datasets\n",
    "        for task in prepare_tasks_func(entry):\n",
    "            doc_id, original_text, summary_to_evaluate, model_name = task\n",
    "\n",
    "            print(f\"\\n---> Evaluating '{model_name}' for doc: {doc_id}\")\n",
    "\n",
    "            evaluation_object, usage_data = evaluate_extraction(\n",
    "                original_text,\n",
    "                summary_to_evaluate,\n",
    "                prompt_template,\n",
    "                pydantic_model\n",
    "            )\n",
    "\n",
    "            if evaluation_object and usage_data:\n",
    "                total_tokens += usage_data.total_tokens\n",
    "                result_record = {\n",
    "                    \"document_idx\": doc_id,\n",
    "                    \"model_evaluated\": model_name,\n",
    "                    \"evaluation_data\": evaluation_object.model_dump(),\n",
    "                    \"token_usage\": {\n",
    "                        \"prompt_tokens\": usage_data.prompt_tokens,\n",
    "                        \"completion_tokens\": usage_data.completion_tokens,\n",
    "                        \"total_tokens\": usage_data.total_tokens\n",
    "                    },\n",
    "                }\n",
    "                all_evaluations.append(result_record)\n",
    "                print(f\"  Result stored. Used tokens: {usage_data.total_tokens}\")\n",
    "\n",
    "    return all_evaluations, total_tokens"
   ],
   "id": "f22c257f27f8d81f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6. --- EXECUTION BLOCK ---",
   "id": "ed2ff7fad5ba4221"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T02:33:29.136232Z",
     "start_time": "2025-07-29T02:32:50.545651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- Configure what you want to execute here ---\n",
    "    # Options: \"BASSE\" or \"FLARES\"\n",
    "    EVALUATION_TARGET = \"FLARES\"\n",
    "\n",
    "    # --- EVALUATIONS PARAMETERS ---\n",
    "    BASSE_DATASET_FILEPATH = str(PROJECT_ROOT / 'data' / 'basse' / 'BASSE.jsonl')\n",
    "    FLARES_DATASET_FILEPATHS = [\n",
    "        str(PROJECT_ROOT / 'data' / 'flares' / '5w1h_subtarea_1_train.json'),\n",
    "        str(PROJECT_ROOT / 'data' / 'flares' / '5w1h_subtask_1_trial.json')\n",
    "    ]\n",
    "    PROMPT_FILE = str(PROJECT_ROOT / 'prompts' / 'evaluation_prompt_v3.txt')\n",
    "    # Make sure the Pydantic model to use is defined or imported\n",
    "    PYDANTIC_MODEL = DetailedEvaluation\n",
    "\n",
    "    dataset_to_run = None\n",
    "    task_preparer = None\n",
    "    output_filename = None\n",
    "\n",
    "    print(f\"Starting evaluation for target: {EVALUATION_TARGET}\")\n",
    "\n",
    "    if EVALUATION_TARGET == \"BASSE\":\n",
    "        dataset_to_run = load_basse_dataset(BASSE_DATASET_FILEPATH)\n",
    "        task_preparer = prepare_basse_tasks\n",
    "    elif EVALUATION_TARGET == \"FLARES\":\n",
    "        dataset_to_run = load_flares_dataset(FLARES_DATASET_FILEPATHS)\n",
    "        task_preparer = prepare_flares_tasks\n",
    "\n",
    "    if dataset_to_run and task_preparer:\n",
    "        final_results, total_tokens = process_dataset(\n",
    "            dataset=dataset_to_run,\n",
    "            prepare_tasks_func=task_preparer,\n",
    "            prompt_path=PROMPT_FILE,\n",
    "            pydantic_model=PYDANTIC_MODEL\n",
    "        )\n",
    "\n",
    "        # --- Create the dynamic filename ---\n",
    "\n",
    "        # 1. Get the current date\n",
    "        current_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "        # 2. Get other variables (ensure they are loaded from os.getenv earlier)\n",
    "        environment = os.getenv(\"ENVIRONMENT\", \"development\")\n",
    "\n",
    "        # 3. Sanitize model name to be filename-friendly (replaces slashes)\n",
    "        safe_model_name = model_name.replace('/', '_')\n",
    "\n",
    "        # 4. Assemble the final filename\n",
    "        output_filename = f\"results/{current_date}_{environment}_{EVALUATION_TARGET}_{ai_provider}_{safe_model_name}.json\"\n",
    "\n",
    "        # --- Export the data to the new file ---\n",
    "\n",
    "        export_data = {\"total_tokens\": total_tokens, \"results\": final_results}\n",
    "\n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"\\nResults exported to: {output_filename}\")\n",
    "    else:\n",
    "        print(\"Evaluation target not found or dataset could not be loaded.\")\n"
   ],
   "id": "73618fa8c57a32fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation for target: FLARES\n",
      "Processed and merged 1753 objects from 2 file(s).\n",
      "Example of the first object in 'merged_dataset':\n",
      "{\n",
      "  \"Id\": 732,\n",
      "  \"Text\": \"Dos días, exactamente han pasado dos días desde que Sánchez compareciera en rueda de prensa en la Moncloa afirmando que a España llegarían, entre abril y septiembre, un total de 87 millones de vacunas para darnos cuenta de que las mentiras de Sánchez hacen bueno ese refrán que dice que “la mentira tiene las patas muy cortas”.\",\n",
      "  \"Processed_Tags\": [\n",
      "    {\n",
      "      \"5W1H_Label\": \"WHO\",\n",
      "      \"Enumerated_Tag_Id\": \"WHO_1\",\n",
      "      \"Reliability_Label\": \"confiable\",\n",
      "      \"Tag_Text\": \"Sánchez\",\n",
      "      \"Tag_Start\": 52\n",
      "    },\n",
      "    {\n",
      "      \"5W1H_Label\": \"WHERE\",\n",
      "      \"Enumerated_Tag_Id\": \"WHERE_1\",\n",
      "      \"Reliability_Label\": \"confiable\",\n",
      "      \"Tag_Text\": \"en rueda de prensa en la Moncloa\",\n",
      "      \"Tag_Start\": 73\n",
      "    },\n",
      "    {\n",
      "      \"5W1H_Label\": \"WHO\",\n",
      "      \"Enumerated_Tag_Id\": \"WHO_2\",\n",
      "      \"Reliability_Label\": \"confiable\",\n",
      "      \"Tag_Text\": \"a España\",\n",
      "      \"Tag_Start\": 120\n",
      "    },\n",
      "    {\n",
      "      \"5W1H_Label\": \"WHEN\",\n",
      "      \"Enumerated_Tag_Id\": \"WHEN_1\",\n",
      "      \"Reliability_Label\": \"confiable\",\n",
      "      \"Tag_Text\": \"entre abril y septiembre\",\n",
      "      \"Tag_Start\": 140\n",
      "    },\n",
      "    {\n",
      "      \"5W1H_Label\": \"WHAT\",\n",
      "      \"Enumerated_Tag_Id\": \"WHAT_1\",\n",
      "      \"Reliability_Label\": \"confiable\",\n",
      "      \"Tag_Text\": \"un total de 87 millones de vacunas\",\n",
      "      \"Tag_Start\": 166\n",
      "    },\n",
      "    {\n",
      "      \"5W1H_Label\": \"WHAT\",\n",
      "      \"Enumerated_Tag_Id\": \"WHAT_2\",\n",
      "      \"Reliability_Label\": \"no confiable\",\n",
      "      \"Tag_Text\": \"las mentiras\",\n",
      "      \"Tag_Start\": 227\n",
      "    },\n",
      "    {\n",
      "      \"5W1H_Label\": \"WHO\",\n",
      "      \"Enumerated_Tag_Id\": \"WHO_3\",\n",
      "      \"Reliability_Label\": \"confiable\",\n",
      "      \"Tag_Text\": \"de Sánchez\",\n",
      "      \"Tag_Start\": 240\n",
      "    },\n",
      "    {\n",
      "      \"5W1H_Label\": \"WHAT\",\n",
      "      \"Enumerated_Tag_Id\": \"WHAT_3\",\n",
      "      \"Reliability_Label\": \"semiconfiable\",\n",
      "      \"Tag_Text\": \"ese refrán que dice\",\n",
      "      \"Tag_Start\": 263\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "After applying the 'best combination' filter, 94 objects remained.\n",
      "Transformed 94 objects to flat format.\n",
      "\n",
      "Example of the first object in 'final_flat_list':\n",
      "{\n",
      "  \"Id\": 732,\n",
      "  \"Text\": \"Dos días, exactamente han pasado dos días desde que Sánchez compareciera en rueda de prensa en la Moncloa afirmando que a España llegarían, entre abril y septiembre, un total de 87 millones de vacunas para darnos cuenta de que las mentiras de Sánchez hacen bueno ese refrán que dice que “la mentira tiene las patas muy cortas”.\",\n",
      "  \"Who\": \"Sánchez\",\n",
      "  \"Where\": \"en rueda de prensa en la Moncloa\",\n",
      "  \"When\": \"entre abril y septiembre\",\n",
      "  \"What\": \"un total de 87 millones de vacunas\"\n",
      "}\n",
      "Processing 5 documents...\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 732\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 2702\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1229\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 3005\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 840\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 2742\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 397\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 2726\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1523\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 2611\n",
      "\n",
      "Results exported to: results/2025-07-28_22-33-29_development_FLARES_anthropic_claude-3-5-haiku-latest.json\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
