{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. --- IMPORTS AND CONFIGURATION ---",
   "id": "470bace0b6888015"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T20:06:14.148927Z",
     "start_time": "2025-07-16T20:06:13.449533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Importing the custom preprocessing function for BASSE dataset and preparation tasks\n",
    "from preprocessing.basse_preprocessing import process_basse_summaries\n",
    "from preparation.basse_preparation import prepare_basse_tasks\n",
    "\n",
    "# Importing the custom preprocessing functions for FLARES dataset and preparation tasks\n",
    "from preprocessing.flares_preprocessing import load_and_merge_datasets, process_and_flatten_data\n",
    "from preparation.flares_preparation import prepare_flares_tasks\n",
    "\n",
    "# Define the Pydantic models for structured output\n",
    "from pydantic_models.basse_pydantic_models import DetailedEvaluation\n",
    "\n",
    "# Load environment variables (will look for .env file)\n",
    "load_dotenv()\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "# Initialize the OpenAI client.\n",
    "# It will automatically read the OPENAI_API_KEY environment variable\n",
    "try:\n",
    "    client = openai.OpenAI()\n",
    "except openai.OpenAIError as e:\n",
    "    print(\"Error initializing the OpenAI client. Please ensure the OPENAI_API_KEY environment variable is configured.\")\n",
    "    print(e)\n",
    "    # Exit or handle the error appropriately\n",
    "    exit()"
   ],
   "id": "9fb2a029de32d32b",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. --- LOAD DATASETS ---",
   "id": "eebe3b8f481fa398"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T20:06:14.176528Z",
     "start_time": "2025-07-16T20:06:14.171299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_basse_dataset(basse_path: str, ):\n",
    "    \"\"\"\n",
    "    Loads the BASSE dataset from a JSONL file and processes it.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return process_basse_summaries(basse_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The BASSE dataset file was not found at path '{basse_path}'\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def load_flares_dataset(flares_path: list):\n",
    "    \"\"\"\n",
    "    Loads and processes the FLARES dataset from multiple JSON files.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load and merge the datasets\n",
    "        flares_datasets_merged = load_and_merge_datasets(flares_path)\n",
    "        # Process and flatten the data\n",
    "        return process_and_flatten_data(flares_datasets_merged)\n",
    "    except FileNotFoundError:\n",
    "        print(\n",
    "            f\"Error: The FLARES dataset files were not found at the specified paths: {flares_path[0]} and {flares_path[1]}\")\n",
    "        return []"
   ],
   "id": "50fa6f2717881f0",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. --- FUNCTION TO LOAD THE PROMPT ---",
   "id": "b9ba32a8ffff78dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T20:06:14.197877Z",
     "start_time": "2025-07-16T20:06:14.193542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_prompt_from_file(filepath: str) -> str:\n",
    "    \"\"\"Reads and returns the content of a text file.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The prompt file was not found at path '{filepath}'\")\n",
    "        return \"\"  # Return an empty string in case of an error"
   ],
   "id": "aa04cd91dbb0ba75",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. --- EVALUATION FUNCTION WITH STRUCTURED OUTPUT ---",
   "id": "850f8f3053ad599f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T20:06:14.387040Z",
     "start_time": "2025-07-16T20:06:14.380109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_extraction(document: str, extraction: str, prompt_template: str, pydantic_model: BaseModel) -> tuple:\n",
    "    \"\"\"\n",
    "    Prepares and calls the OpenAI API using 'response_model' for structured and validated output.\n",
    "    \"\"\"\n",
    "    if not prompt_template:\n",
    "        print(\"Cannot proceed without a prompt template.\")\n",
    "        return None, None\n",
    "\n",
    "    filled_prompt = prompt_template.format(\n",
    "        original_document=document,\n",
    "        extraction_to_evaluate=extraction\n",
    "    )\n",
    "\n",
    "    print(\"--- STARTING STRUCTURED EVALUATION ---\")\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=os.environ.get(\"OPENAI_MODEL\", \"gpt-4.1-nano\"),  # Default to gpt-4.1-nano if no key is set\n",
    "            messages=[{\"role\": \"user\", \"content\": filled_prompt}],\n",
    "            # 1. We define the \"tool\" that the model should use.\n",
    "            #    We use .model_json_schema() to automatically generate the schema.\n",
    "            tools=[\n",
    "                {\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": \"save_evaluation\",\n",
    "                        \"description\": \"Saves the structured evaluation result of an extraction.\",\n",
    "                        \"parameters\": pydantic_model.model_json_schema()\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            # 2. We force the model to call our tool.\n",
    "            tool_choice={\"type\": \"function\", \"function\": {\"name\": \"save_evaluation\"}}\n",
    "        )\n",
    "\n",
    "        # 3. We extract the result from the tool call arguments.\n",
    "        #    The response is a JSON string, not an object.\n",
    "        tool_call = response.choices[0].message.tool_calls[0]\n",
    "        json_arguments = tool_call.function.arguments\n",
    "\n",
    "        # We capture tokens' usage\n",
    "        token_usage = response.usage\n",
    "\n",
    "        # 4. We parse and validate the JSON against our Pydantic model.\n",
    "        evaluation_object = pydantic_model.model_validate_json(json_arguments)\n",
    "\n",
    "        return evaluation_object, token_usage\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred calling the API or processing the response: {e}\")\n",
    "        return None, None"
   ],
   "id": "a04d9c9294cfe927",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. --- MAIN ITERATION AND STORAGE LOGIC ---",
   "id": "ceec2ddf7ee558d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T20:06:14.409166Z",
     "start_time": "2025-07-16T20:06:14.402162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_dataset(\n",
    "        dataset: list,\n",
    "        prepare_tasks_func: callable,\n",
    "        prompt_path: str,\n",
    "        pydantic_model: BaseModel\n",
    "):\n",
    "    \"\"\"\n",
    "    A generic main loop to process any dataset using a specific task preparer.\n",
    "    \"\"\"\n",
    "    prompt_template = load_prompt_from_file(prompt_path)\n",
    "    if not prompt_template:\n",
    "        return [], 0\n",
    "\n",
    "    all_evaluations = []\n",
    "    total_tokens = 0\n",
    "    env = os.environ.get(\"ENVIRONMENT\", \"development\")\n",
    "    docs_to_process = dataset[:5] if env == \"development\" else dataset\n",
    "\n",
    "    print(f\"Processing {len(docs_to_process)} documents...\")\n",
    "\n",
    "    for entry in docs_to_process:\n",
    "        # The preparer function handles the differences between datasets\n",
    "        for task in prepare_tasks_func(entry):\n",
    "            doc_id, original_text, summary_to_evaluate, model_name = task\n",
    "\n",
    "            print(f\"\\n---> Evaluating '{model_name}' for doc: {doc_id}\")\n",
    "\n",
    "            evaluation_object, usage_data = evaluate_extraction(\n",
    "                original_text,\n",
    "                summary_to_evaluate,\n",
    "                prompt_template,\n",
    "                pydantic_model\n",
    "            )\n",
    "\n",
    "            if evaluation_object and usage_data:\n",
    "                total_tokens += usage_data.total_tokens\n",
    "                result_record = {\n",
    "                    \"document_idx\": doc_id,\n",
    "                    \"model_evaluated\": model_name,\n",
    "                    \"evaluation_data\": evaluation_object.model_dump(),\n",
    "                    \"token_usage\": {\n",
    "                        \"prompt_tokens\": usage_data.prompt_tokens,\n",
    "                        \"completion_tokens\": usage_data.completion_tokens,\n",
    "                        \"total_tokens\": usage_data.total_tokens\n",
    "                    },\n",
    "                }\n",
    "                all_evaluations.append(result_record)\n",
    "                print(f\"  Result stored. Used tokens: {usage_data.total_tokens}\")\n",
    "\n",
    "    return all_evaluations, total_tokens"
   ],
   "id": "f22c257f27f8d81f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. --- EXECUTION BLOCK ---",
   "id": "ed2ff7fad5ba4221"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T20:42:33.436893Z",
     "start_time": "2025-07-16T20:34:24.885323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- Configure what you want to execute here ---\n",
    "    # Options: \"BASSE\" or \"FLARES\"\n",
    "    EVALUATION_TARGET = \"FLARES\"\n",
    "\n",
    "    # --- EVALUATIONS PARAMETERS ---\n",
    "    BASSE_DATASET_FILEPATH = str(PROJECT_ROOT / 'data' / 'basse' / 'BASSE.jsonl')\n",
    "    FLARES_DATASET_FILEPATHS = [\n",
    "        str(PROJECT_ROOT / 'data' / 'flares' / '5w1h_subtarea_1_train.json'),\n",
    "        str(PROJECT_ROOT / 'data' / 'flares' / '5w1h_subtask_1_trial.json')\n",
    "    ]\n",
    "    PROMPT_FILE = str(PROJECT_ROOT / 'prompts' / 'evaluation_prompt_v3.txt')\n",
    "    # Make sure the Pydantic model to use is defined or imported\n",
    "    PYDANTIC_MODEL = DetailedEvaluation\n",
    "\n",
    "    dataset_to_run = None\n",
    "    task_preparer = None\n",
    "    output_filename = None\n",
    "\n",
    "    print(f\"Starting evaluation for target: {EVALUATION_TARGET}\")\n",
    "\n",
    "    if EVALUATION_TARGET == \"BASSE\":\n",
    "        dataset_to_run = load_basse_dataset(BASSE_DATASET_FILEPATH)\n",
    "        task_preparer = prepare_basse_tasks\n",
    "        output_filename = \"results/evaluation_results_basse.json\"\n",
    "    elif EVALUATION_TARGET == \"FLARES\":\n",
    "        dataset_to_run = load_flares_dataset(FLARES_DATASET_FILEPATHS)\n",
    "        task_preparer = prepare_flares_tasks\n",
    "        output_filename = \"results/evaluation_results_flares.json\"\n",
    "\n",
    "    if dataset_to_run and task_preparer:\n",
    "        final_results, total_tokens = process_dataset(\n",
    "            dataset=dataset_to_run,\n",
    "            prepare_tasks_func=task_preparer,\n",
    "            prompt_path=PROMPT_FILE,\n",
    "            pydantic_model=PYDANTIC_MODEL\n",
    "        )\n",
    "\n",
    "        print(\"\\n\\n####################################################\")\n",
    "        print(f\"####### {EVALUATION_TARGET} EVALUATION FINISHED #######\")\n",
    "        print(f\"         Total used tokens: {total_tokens}          \")\n",
    "        print(\"####################################################\")\n",
    "\n",
    "        export_data = {\"total_tokens\": total_tokens, \"results\": final_results}\n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"\\nResults exported to: {output_filename}\")\n",
    "    else:\n",
    "        print(\"Evaluation target not found or dataset could not be loaded.\")\n"
   ],
   "id": "73618fa8c57a32fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation for target: FLARES\n",
      "Processed and merged 1753 objects from 2 file(s).\n",
      "Example of the first object in 'merged_dataset':\n",
      "{\n",
      "  \"Id\": 732,\n",
      "  \"Text\": \"Dos días, exactamente han pasado dos días desde que Sánchez compareciera en rueda de prensa en la Moncloa afirmando que a España llegarían, entre abril y septiembre, un total de 87 millones de vacunas para darnos cuenta de que las mentiras de Sánchez hacen bueno ese refrán que dice que “la mentira tiene las patas muy cortas”.\",\n",
      "  \"Processed_Tags\": [\n",
      "    {\n",
      "      \"5W1H_Label\": \"WHO\",\n",
      "      \"Enumerated_Tag_Id\": \"WHO_1\",\n",
      "      \"Reliability_Label\": \"confiable\",\n",
      "      \"Tag_Text\": \"Sánchez\",\n",
      "      \"Tag_Start\": 52\n",
      "    },\n",
      "    {\n",
      "      \"5W1H_Label\": \"WHERE\",\n",
      "      \"Enumerated_Tag_Id\": \"WHERE_1\",\n",
      "      \"Reliability_Label\": \"confiable\",\n",
      "      \"Tag_Text\": \"en rueda de prensa en la Moncloa\",\n",
      "      \"Tag_Start\": 73\n",
      "    },\n",
      "    {\n",
      "      \"5W1H_Label\": \"WHO\",\n",
      "      \"Enumerated_Tag_Id\": \"WHO_2\",\n",
      "      \"Reliability_Label\": \"confiable\",\n",
      "      \"Tag_Text\": \"a España\",\n",
      "      \"Tag_Start\": 120\n",
      "    },\n",
      "    {\n",
      "      \"5W1H_Label\": \"WHEN\",\n",
      "      \"Enumerated_Tag_Id\": \"WHEN_1\",\n",
      "      \"Reliability_Label\": \"confiable\",\n",
      "      \"Tag_Text\": \"entre abril y septiembre\",\n",
      "      \"Tag_Start\": 140\n",
      "    },\n",
      "    {\n",
      "      \"5W1H_Label\": \"WHAT\",\n",
      "      \"Enumerated_Tag_Id\": \"WHAT_1\",\n",
      "      \"Reliability_Label\": \"confiable\",\n",
      "      \"Tag_Text\": \"un total de 87 millones de vacunas\",\n",
      "      \"Tag_Start\": 166\n",
      "    },\n",
      "    {\n",
      "      \"5W1H_Label\": \"WHAT\",\n",
      "      \"Enumerated_Tag_Id\": \"WHAT_2\",\n",
      "      \"Reliability_Label\": \"no confiable\",\n",
      "      \"Tag_Text\": \"las mentiras\",\n",
      "      \"Tag_Start\": 227\n",
      "    },\n",
      "    {\n",
      "      \"5W1H_Label\": \"WHO\",\n",
      "      \"Enumerated_Tag_Id\": \"WHO_3\",\n",
      "      \"Reliability_Label\": \"confiable\",\n",
      "      \"Tag_Text\": \"de Sánchez\",\n",
      "      \"Tag_Start\": 240\n",
      "    },\n",
      "    {\n",
      "      \"5W1H_Label\": \"WHAT\",\n",
      "      \"Enumerated_Tag_Id\": \"WHAT_3\",\n",
      "      \"Reliability_Label\": \"semiconfiable\",\n",
      "      \"Tag_Text\": \"ese refrán que dice\",\n",
      "      \"Tag_Start\": 263\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "After applying the 'best combination' filter, 94 objects remained.\n",
      "Transformed 94 objects to flat format.\n",
      "\n",
      "Example of the first object in 'final_flat_list':\n",
      "{\n",
      "  \"Id\": 732,\n",
      "  \"Text\": \"Dos días, exactamente han pasado dos días desde que Sánchez compareciera en rueda de prensa en la Moncloa afirmando que a España llegarían, entre abril y septiembre, un total de 87 millones de vacunas para darnos cuenta de que las mentiras de Sánchez hacen bueno ese refrán que dice que “la mentira tiene las patas muy cortas”.\",\n",
      "  \"Who\": \"Sánchez\",\n",
      "  \"Where\": \"en rueda de prensa en la Moncloa\",\n",
      "  \"When\": \"entre abril y septiembre\",\n",
      "  \"What\": \"un total de 87 millones de vacunas\"\n",
      "}\n",
      "Processing 94 documents...\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 732\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1620\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1229\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1791\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 840\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1577\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 397\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1514\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1523\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1508\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 938\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1566\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 507\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1493\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 2\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1526\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1525\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1513\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 924\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1562\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 479\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1508\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1384\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1527\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1396\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1472\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1471\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1538\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1464\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1550\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1044\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1525\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 522\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1481\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1250\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1517\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 36\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1510\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1564\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1606\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 292\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1484\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 993\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1586\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 279\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1564\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 335\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1639\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 89\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1519\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 922\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1480\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 692\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1556\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 568\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1558\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1462\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1623\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1022\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1467\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 592\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1525\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1552\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1591\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 717\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1692\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 50\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1551\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 710\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1582\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1460\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1518\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 355\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1500\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1356\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1640\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1446\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1548\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 727\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1590\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1152\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1569\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 150\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1636\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 944\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1580\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 90\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1487\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1072\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1589\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1549\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1484\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 689\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1525\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1049\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1576\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 44\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1539\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1146\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1619\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 951\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1591\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1410\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1631\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 979\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1626\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 720\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1599\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 354\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1525\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 287\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1582\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1089\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1527\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 410\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1725\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1405\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1645\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 148\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1642\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1568\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1500\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 398\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1554\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1580\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1764\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 724\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1548\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1524\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1596\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 810\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1526\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 341\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1528\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1381\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1460\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1309\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1520\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 751\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1626\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1581\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1562\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1150\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1465\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 70\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1495\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1284\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1535\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 153\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1517\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 875\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1523\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 765\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1532\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 464\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1509\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 817\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1611\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 359\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1517\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1062\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1549\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 417\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1568\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 347\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1471\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 451\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1537\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1370\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1535\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 236\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1548\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1352\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1593\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 784\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1610\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 423\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1495\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 828\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1497\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 746\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1533\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 109\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1616\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 158\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1488\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 87\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1482\n",
      "\n",
      "\n",
      "####################################################\n",
      "####### FLARES EVALUATION FINISHED #######\n",
      "         Total used tokens: 146224          \n",
      "####################################################\n",
      "\n",
      "Results exported to: evaluation_results_flares.json\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
