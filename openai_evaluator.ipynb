{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. --- IMPORTS AND CONFIGURATION ---",
   "id": "470bace0b6888015"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T13:48:31.153149Z",
     "start_time": "2025-08-04T13:48:30.566742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Importing the custom preprocessing function for BASSE dataset and preparation tasks\n",
    "from preprocessing.basse_preprocessing import process_basse_summaries\n",
    "from preparation.basse_preparation import prepare_basse_tasks\n",
    "\n",
    "# Importing the custom preprocessing functions for FLARES dataset and preparation tasks\n",
    "from preprocessing.flares_preprocessing import load_and_merge_datasets, process_and_flatten_data\n",
    "from preparation.flares_preparation import prepare_flares_tasks\n",
    "\n",
    "# Define the Pydantic models for structured output\n",
    "from pydantic_models.basse_pydantic_models import DetailedEvaluation\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "# Load environment variables (will look for .env file)\n",
    "load_dotenv()\n",
    "\n",
    "# --- Dynamic AI Provider Configuration ---\n",
    "\n",
    "# Read environment variables\n",
    "ai_provider = os.getenv(\"MODEL_PROVIDER\", \"openai\").lower() # Default to 'openai'\n",
    "model_name = os.getenv(\"MODEL\")\n",
    "\n",
    "api_key = None\n",
    "base_url = None\n",
    "\n",
    "print(f\"Using provider: {ai_provider.capitalize()}\")\n",
    "\n",
    "if ai_provider == \"openai\":\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    # base_url is not needed; the client defaults to OpenAI's endpoint.\n",
    "elif ai_provider == \"gemini\":\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "elif ai_provider == \"anthropic\":\n",
    "    api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "    base_url = \"https://api.anthropic.com/v1/\"\n",
    "elif ai_provider == \"openrouter\":\n",
    "    api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "    base_url = \"https://openrouter.ai/api/v1\"\n",
    "else:\n",
    "    print(f\"Error: AI provider '{ai_provider}' is not supported. Options: openai, gemini, anthropic.\")\n",
    "    exit()\n",
    "\n",
    "# --- Client Initialization ---\n",
    "\n",
    "# Validate that the API key and model are defined\n",
    "if not api_key:\n",
    "    print(f\"Error: API key environment variable for '{ai_provider}' is not set.\")\n",
    "    exit()\n",
    "if not model_name:\n",
    "    print(\"Error: 'MODEL' environment variable is not set.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize the OpenAI client with the dynamic configuration\n",
    "try:\n",
    "    client = openai.OpenAI(api_key=api_key, base_url=base_url)\n",
    "except openai.OpenAIError as e:\n",
    "    print(f\"Error initializing the client for {ai_provider.capitalize()}.\")\n",
    "    print(e)\n",
    "    exit()\n",
    "\n",
    "# Now you can use the client and model name in your API calls\n",
    "print(f\"✅ Client initialized successfully. Model to use: {model_name}\")"
   ],
   "id": "9fb2a029de32d32b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provider: Openrouter\n",
      "✅ Client initialized successfully. Model to use: openrouter/horizon-beta\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. --- LOAD DATASETS ---",
   "id": "eebe3b8f481fa398"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T13:48:31.184546Z",
     "start_time": "2025-08-04T13:48:31.178505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_basse_dataset(basse_path: str, ):\n",
    "    \"\"\"\n",
    "    Loads the BASSE dataset from a JSONL file and processes it.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return process_basse_summaries(basse_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The BASSE dataset file was not found at path '{basse_path}'\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def load_flares_dataset(flares_path: list):\n",
    "    \"\"\"\n",
    "    Loads and processes the FLARES dataset from multiple JSON files.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load and merge the datasets\n",
    "        flares_datasets_merged = load_and_merge_datasets(flares_path)\n",
    "        # Process and flatten the data\n",
    "        return process_and_flatten_data(flares_datasets_merged)\n",
    "    except FileNotFoundError:\n",
    "        print(\n",
    "            f\"Error: The FLARES dataset files were not found at the specified paths: {flares_path[0]} and {flares_path[1]}\")\n",
    "        return []"
   ],
   "id": "50fa6f2717881f0",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. --- FUNCTION TO LOAD THE PROMPT ---",
   "id": "b9ba32a8ffff78dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T13:48:31.360751Z",
     "start_time": "2025-08-04T13:48:31.353094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_prompt_from_file(filepath: str) -> str:\n",
    "    \"\"\"Reads and returns the content of a text file.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The prompt file was not found at path '{filepath}'\")\n",
    "        return \"\"  # Return an empty string in case of an error"
   ],
   "id": "aa04cd91dbb0ba75",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. --- EVALUATION FUNCTION WITH STRUCTURED OUTPUT ---",
   "id": "850f8f3053ad599f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T13:48:31.391676Z",
     "start_time": "2025-08-04T13:48:31.384205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_extraction(document: str, extraction: str, prompt_template: str, pydantic_model: BaseModel) -> tuple:\n",
    "    \"\"\"\n",
    "    Prepares and calls the OpenAI API using 'response_model' for structured and validated output.\n",
    "    \"\"\"\n",
    "    if not prompt_template:\n",
    "        print(\"Cannot proceed without a prompt template.\")\n",
    "        return None, None\n",
    "\n",
    "    filled_prompt = prompt_template.format(\n",
    "        original_document=document,\n",
    "        extraction_to_evaluate=extraction\n",
    "    )\n",
    "\n",
    "    print(\"--- STARTING STRUCTURED EVALUATION ---\")\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": filled_prompt}],\n",
    "            # 1. We define the \"tool\" that the model should use.\n",
    "            #    We use .model_json_schema() to automatically generate the schema.\n",
    "            tools=[\n",
    "                {\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": \"save_evaluation\",\n",
    "                        \"description\": \"Saves the structured evaluation result of an extraction.\",\n",
    "                        \"parameters\": pydantic_model.model_json_schema()\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            # 2. We force the model to call our tool.\n",
    "            tool_choice={\"type\": \"function\", \"function\": {\"name\": \"save_evaluation\"}}\n",
    "        )\n",
    "\n",
    "        # 3. We extract the result from the tool call arguments.\n",
    "        #    The response is a JSON string, not an object.\n",
    "        tool_call = response.choices[0].message.tool_calls[0]\n",
    "        json_arguments = tool_call.function.arguments\n",
    "\n",
    "        # We capture tokens' usage\n",
    "        token_usage = response.usage\n",
    "\n",
    "        # 4. We parse and validate the JSON against our Pydantic model.\n",
    "        evaluation_object = pydantic_model.model_validate_json(json_arguments)\n",
    "\n",
    "        return evaluation_object, token_usage\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred calling the API or processing the response: {e}\")\n",
    "        return None, None"
   ],
   "id": "a04d9c9294cfe927",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. --- MAIN ITERATION AND STORAGE LOGIC ---",
   "id": "ceec2ddf7ee558d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T13:48:31.417968Z",
     "start_time": "2025-08-04T13:48:31.409915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_dataset(\n",
    "        dataset: list,\n",
    "        prepare_tasks_func: callable,\n",
    "        prompt_path: str,\n",
    "        pydantic_model: BaseModel\n",
    "):\n",
    "    \"\"\"\n",
    "    A generic main loop to process any dataset using a specific task preparer.\n",
    "    \"\"\"\n",
    "    prompt_template = load_prompt_from_file(prompt_path)\n",
    "    if not prompt_template:\n",
    "        return [], 0\n",
    "\n",
    "    all_evaluations = []\n",
    "    total_tokens = 0\n",
    "    env = os.environ.get(\"ENVIRONMENT\", \"development\")\n",
    "    docs_to_process = dataset[:5] if env == \"development\" else dataset\n",
    "\n",
    "    print(f\"Processing {len(docs_to_process)} documents...\")\n",
    "\n",
    "    for entry in docs_to_process:\n",
    "        # The preparer function handles the differences between datasets\n",
    "        for task in prepare_tasks_func(entry):\n",
    "            doc_id, original_text, summary_to_evaluate, model_name = task\n",
    "\n",
    "            print(f\"\\n---> Evaluating '{model_name}' for doc: {doc_id}\")\n",
    "\n",
    "            evaluation_object, usage_data = evaluate_extraction(\n",
    "                original_text,\n",
    "                summary_to_evaluate,\n",
    "                prompt_template,\n",
    "                pydantic_model\n",
    "            )\n",
    "\n",
    "            if evaluation_object and usage_data:\n",
    "                total_tokens += usage_data.total_tokens\n",
    "                result_record = {\n",
    "                    \"document_idx\": doc_id,\n",
    "                    \"model_evaluated\": model_name,\n",
    "                    \"evaluation_data\": evaluation_object.model_dump(),\n",
    "                    \"token_usage\": {\n",
    "                        \"prompt_tokens\": usage_data.prompt_tokens,\n",
    "                        \"completion_tokens\": usage_data.completion_tokens,\n",
    "                        \"total_tokens\": usage_data.total_tokens\n",
    "                    },\n",
    "                }\n",
    "                all_evaluations.append(result_record)\n",
    "                print(f\"  Result stored. Used tokens: {usage_data.total_tokens}\")\n",
    "\n",
    "    return all_evaluations, total_tokens"
   ],
   "id": "f22c257f27f8d81f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6. --- EXECUTION BLOCK ---",
   "id": "ed2ff7fad5ba4221"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T13:59:00.022192Z",
     "start_time": "2025-08-04T13:48:31.441818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- Configure what you want to execute here ---\n",
    "    # Options: \"BASSE\" or \"FLARES\"\n",
    "    evaluation_dataset = os.environ.get(\"EVALUATION_DATASET\", \"BASSE\").upper()\n",
    "\n",
    "    if not evaluation_dataset:\n",
    "        print(f\"Error: 'EVALUATION_DATASET' environment variable is not set.\")\n",
    "        exit()\n",
    "    if evaluation_dataset not in [\"BASSE\", \"FLARES\"]:\n",
    "        print(f\"Error: 'EVALUATION_DATASET' must be either 'BASSE' or 'FLARES'.\")\n",
    "        exit()\n",
    "\n",
    "    # --- EVALUATIONS PARAMETERS ---\n",
    "    BASSE_DATASET_FILEPATH = str(PROJECT_ROOT / 'data' / 'basse' / 'BASSE.jsonl')\n",
    "    FLARES_DATASET_FILEPATHS = [\n",
    "        str(PROJECT_ROOT / 'data' / 'flares' / '5w1h_subtarea_1_train.json'),\n",
    "        str(PROJECT_ROOT / 'data' / 'flares' / '5w1h_subtask_1_trial.json')\n",
    "    ]\n",
    "    PROMPT_FILE = str(PROJECT_ROOT / 'prompts' / 'evaluation_prompt_v3.txt')\n",
    "    # Make sure the Pydantic model to use is defined or imported\n",
    "    PYDANTIC_MODEL = DetailedEvaluation\n",
    "\n",
    "    dataset_to_run = None\n",
    "    task_preparer = None\n",
    "    output_filename = None\n",
    "\n",
    "    print(f\"Starting evaluation for target: {evaluation_dataset}\")\n",
    "\n",
    "    if evaluation_dataset == \"BASSE\":\n",
    "        dataset_to_run = load_basse_dataset(BASSE_DATASET_FILEPATH)\n",
    "        task_preparer = prepare_basse_tasks\n",
    "    elif evaluation_dataset == \"FLARES\":\n",
    "        dataset_to_run = load_flares_dataset(FLARES_DATASET_FILEPATHS)\n",
    "        task_preparer = prepare_flares_tasks\n",
    "\n",
    "    if dataset_to_run and task_preparer:\n",
    "        final_results, total_tokens = process_dataset(\n",
    "            dataset=dataset_to_run,\n",
    "            prepare_tasks_func=task_preparer,\n",
    "            prompt_path=PROMPT_FILE,\n",
    "            pydantic_model=PYDANTIC_MODEL\n",
    "        )\n",
    "\n",
    "        # --- Create the dynamic filename ---\n",
    "\n",
    "        # 1. Get the current date\n",
    "        current_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "        # 2. Get other variables (ensure they are loaded from os.getenv earlier)\n",
    "        environment = os.getenv(\"ENVIRONMENT\", \"development\")\n",
    "\n",
    "        # 3. Sanitize model name to be filename-friendly (replaces slashes)\n",
    "        safe_model_name = model_name.replace('/', '_')\n",
    "\n",
    "        # 4. Assemble the final filename\n",
    "        output_filename = f\"results/{current_date}_{environment}_{evaluation_dataset}_{ai_provider}_{safe_model_name}.json\"\n",
    "\n",
    "        # --- Export the data to the new file ---\n",
    "\n",
    "        export_data = {\"total_tokens\": total_tokens, \"results\": final_results}\n",
    "\n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"\\nResults exported to: {output_filename}\")\n",
    "    else:\n",
    "        print(\"Evaluation target not found or dataset could not be loaded.\")\n"
   ],
   "id": "73618fa8c57a32fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation for target: FLARES\n",
      "Processed and merged 1753 objects from 2 file(s).\n",
      "Example of the first object in 'merged_dataset':\n",
      "{\n",
      "  \"Id\": 732,\n",
      "  \"Text\": \"Dos días, exactamente han pasado dos días desde que Sánchez compareciera en rueda de prensa en la Moncloa afirmando que a España llegarían, entre abril y septiembre, un total de 87 millones de vacunas para darnos cuenta de que las mentiras de Sánchez hacen bueno ese refrán que dice que “la mentira tiene las patas muy cortas”.\",\n",
      "  \"Processed_Tags\": [\n",
      "    {\n",
      "      \"5W1H_Label\": \"WHO\",\n",
      "      \"Enumerated_Tag_Id\": \"WHO_1\",\n",
      "      \"Reliability_Label\": \"confiable\",\n",
      "      \"Tag_Text\": \"Sánchez\",\n",
      "      \"Tag_Start\": 52\n",
      "    },\n",
      "    {\n",
      "      \"5W1H_Label\": \"WHERE\",\n",
      "      \"Enumerated_Tag_Id\": \"WHERE_1\",\n",
      "      \"Reliability_Label\": \"confiable\",\n",
      "      \"Tag_Text\": \"en rueda de prensa en la Moncloa\",\n",
      "      \"Tag_Start\": 73\n",
      "    },\n",
      "    {\n",
      "      \"5W1H_Label\": \"WHO\",\n",
      "      \"Enumerated_Tag_Id\": \"WHO_2\",\n",
      "      \"Reliability_Label\": \"confiable\",\n",
      "      \"Tag_Text\": \"a España\",\n",
      "      \"Tag_Start\": 120\n",
      "    },\n",
      "    {\n",
      "      \"5W1H_Label\": \"WHEN\",\n",
      "      \"Enumerated_Tag_Id\": \"WHEN_1\",\n",
      "      \"Reliability_Label\": \"confiable\",\n",
      "      \"Tag_Text\": \"entre abril y septiembre\",\n",
      "      \"Tag_Start\": 140\n",
      "    },\n",
      "    {\n",
      "      \"5W1H_Label\": \"WHAT\",\n",
      "      \"Enumerated_Tag_Id\": \"WHAT_1\",\n",
      "      \"Reliability_Label\": \"confiable\",\n",
      "      \"Tag_Text\": \"un total de 87 millones de vacunas\",\n",
      "      \"Tag_Start\": 166\n",
      "    },\n",
      "    {\n",
      "      \"5W1H_Label\": \"WHAT\",\n",
      "      \"Enumerated_Tag_Id\": \"WHAT_2\",\n",
      "      \"Reliability_Label\": \"no confiable\",\n",
      "      \"Tag_Text\": \"las mentiras\",\n",
      "      \"Tag_Start\": 227\n",
      "    },\n",
      "    {\n",
      "      \"5W1H_Label\": \"WHO\",\n",
      "      \"Enumerated_Tag_Id\": \"WHO_3\",\n",
      "      \"Reliability_Label\": \"confiable\",\n",
      "      \"Tag_Text\": \"de Sánchez\",\n",
      "      \"Tag_Start\": 240\n",
      "    },\n",
      "    {\n",
      "      \"5W1H_Label\": \"WHAT\",\n",
      "      \"Enumerated_Tag_Id\": \"WHAT_3\",\n",
      "      \"Reliability_Label\": \"semiconfiable\",\n",
      "      \"Tag_Text\": \"ese refrán que dice\",\n",
      "      \"Tag_Start\": 263\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "After applying the 'best combination' filter, 94 objects remained.\n",
      "Transformed 94 objects to flat format.\n",
      "\n",
      "Example of the first object in 'final_flat_list':\n",
      "{\n",
      "  \"Id\": 732,\n",
      "  \"Text\": \"Dos días, exactamente han pasado dos días desde que Sánchez compareciera en rueda de prensa en la Moncloa afirmando que a España llegarían, entre abril y septiembre, un total de 87 millones de vacunas para darnos cuenta de que las mentiras de Sánchez hacen bueno ese refrán que dice que “la mentira tiene las patas muy cortas”.\",\n",
      "  \"Who\": \"Sánchez\",\n",
      "  \"Where\": \"en rueda de prensa en la Moncloa\",\n",
      "  \"When\": \"entre abril y septiembre\",\n",
      "  \"What\": \"un total de 87 millones de vacunas\"\n",
      "}\n",
      "Processing 94 documents...\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 732\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1676\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1229\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1987\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 840\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1756\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 397\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1587\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1523\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1554\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 938\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1787\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 507\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1699\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 2\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1662\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1525\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1630\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 924\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1651\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 479\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1648\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1384\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1584\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1396\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1536\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1471\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1628\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1464\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1724\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1044\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1754\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 522\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1647\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1250\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1669\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 36\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1626\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1564\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1771\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 292\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1615\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 993\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1727\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 279\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1605\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 335\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1796\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 89\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1803\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 922\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1617\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 692\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1705\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 568\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1716\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1462\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1698\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1022\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1689\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 592\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1709\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1552\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1762\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 717\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1850\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 50\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1667\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 710\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1733\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1460\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1621\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 355\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1571\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1356\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1700\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1446\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1687\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 727\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1637\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1152\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1603\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 150\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1736\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 944\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1716\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 90\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1716\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1072\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1690\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1549\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1583\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 689\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1728\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1049\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1741\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 44\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1721\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1146\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1622\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 951\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1681\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1410\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1713\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 979\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1734\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 720\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1713\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 354\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1695\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 287\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1640\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1089\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1638\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 410\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1783\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1405\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1760\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 148\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1698\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1568\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1607\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 398\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1669\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1580\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1667\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 724\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1613\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1524\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1758\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 810\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1759\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 341\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1625\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1381\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1714\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1309\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1666\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 751\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1688\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1581\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1735\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1150\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1614\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 70\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1579\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1284\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1762\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 153\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1674\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 875\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1710\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 765\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1733\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 464\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1734\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 817\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1770\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 359\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1767\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1062\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1698\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 417\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1698\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 347\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1626\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 451\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1752\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1370\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1645\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 236\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1661\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 1352\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1756\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 784\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1749\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 423\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1613\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 828\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1672\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 746\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1553\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 109\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1766\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 158\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1607\n",
      "\n",
      "---> Evaluating 'flares_ground_truth' for doc: 87\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 1669\n",
      "\n",
      "Results exported to: results/2025-08-04_09-58-59_production_FLARES_openrouter_openrouter_horizon-beta.json\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
