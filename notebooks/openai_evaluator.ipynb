{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. --- IMPORTS AND CONFIGURATION ---",
   "id": "470bace0b6888015"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:41:33.998219Z",
     "start_time": "2025-10-29T15:41:33.036137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Importing the custom preprocessing function for BASSE dataset and preparation tasks\n",
    "from preprocessing.basse_preprocessing import process_basse_summaries\n",
    "from preparation.basse_preparation import prepare_basse_tasks\n",
    "\n",
    "# Importing the custom preprocessing functions for FLARES dataset and preparation tasks\n",
    "from preprocessing.flares_preprocessing import load_and_merge_datasets, process_and_flatten_data\n",
    "from preparation.flares_preparation import prepare_flares_tasks\n",
    "\n",
    "# Define the Pydantic models for structured output\n",
    "from pydantic_models.output_pydantic_models import DetailedEvaluation\n",
    "\n",
    "# Import the helper function to create the expert review task structure\n",
    "from validation.create_expert_review_task import create_expert_review_task\n",
    "\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "# Load environment variables (will look for .env file)\n",
    "load_dotenv()\n",
    "\n",
    "# --- Dynamic AI Provider Configuration ---\n",
    "\n",
    "# Read environment variables\n",
    "ai_provider = os.getenv(\"MODEL_PROVIDER\", \"openai\").lower()  # Default to 'openai'\n",
    "model_name = os.getenv(\"MODEL\", \"gpt-5-mini\") # Default to 'gpt-5-mini'\n",
    "\n",
    "api_key = None\n",
    "base_url = None\n",
    "\n",
    "print(f\"Using provider: {ai_provider.capitalize()}\")\n",
    "\n",
    "if ai_provider == \"openai\":\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    # base_url is not needed; the client defaults to OpenAI's endpoint.\n",
    "elif ai_provider == \"gemini\":\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "elif ai_provider == \"anthropic\":\n",
    "    api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "    base_url = \"https://api.anthropic.com/v1/\"\n",
    "elif ai_provider == \"openrouter\":\n",
    "    api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "    base_url = \"https://openrouter.ai/api/v1\"\n",
    "else:\n",
    "    print(f\"Error: AI provider '{ai_provider}' is not supported. Options: openai, gemini, anthropic.\")\n",
    "    exit()\n",
    "\n",
    "# --- Client Initialization ---\n",
    "\n",
    "# Validate that the API key and model are defined\n",
    "if not api_key:\n",
    "    print(f\"Error: API key environment variable for '{ai_provider}' is not set.\")\n",
    "    exit()\n",
    "if not model_name:\n",
    "    print(\"Error: 'MODEL' environment variable is not set.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize the OpenAI client with the dynamic configuration\n",
    "try:\n",
    "    client = openai.OpenAI(api_key=api_key, base_url=base_url)\n",
    "except openai.OpenAIError as e:\n",
    "    print(f\"Error initializing the client for {ai_provider.capitalize()}.\")\n",
    "    print(e)\n",
    "    exit()\n",
    "\n",
    "# Now you can use the client and model name in your API calls\n",
    "print(f\"✅ Client initialized successfully. Model to use: {model_name}\")"
   ],
   "id": "9fb2a029de32d32b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provider: Openai\n",
      "✅ Client initialized successfully. Model to use: gpt-5-mini\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. --- LOAD DATASETS ---",
   "id": "eebe3b8f481fa398"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:41:34.049346Z",
     "start_time": "2025-10-29T15:41:34.042309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_basse_dataset(basse_path: str, ):\n",
    "    \"\"\"\n",
    "    Loads the BASSE dataset from a JSONL file and processes it.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return process_basse_summaries(basse_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The BASSE dataset file was not found at path '{basse_path}'\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def load_flares_dataset(flares_path: list):\n",
    "    \"\"\"\n",
    "    Loads and processes the FLARES dataset from multiple JSON files.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load and merge the datasets\n",
    "        flares_datasets_merged = load_and_merge_datasets(flares_path)\n",
    "        # Process and flatten the data\n",
    "        return process_and_flatten_data(flares_datasets_merged)\n",
    "    except FileNotFoundError:\n",
    "        print(\n",
    "            f\"Error: The FLARES dataset files were not found at the specified paths: {flares_path[0]} and {flares_path[1]}\")\n",
    "        return []"
   ],
   "id": "50fa6f2717881f0",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. --- FUNCTION TO LOAD THE PROMPT ---",
   "id": "b9ba32a8ffff78dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:41:34.067092Z",
     "start_time": "2025-10-29T15:41:34.062023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_prompts_from_files(system_prompt_filepath: str, user_prompt_template_filepath) -> (str, str):\n",
    "    \"\"\"Reads and returns the content of a text file.\"\"\"\n",
    "    try:\n",
    "        with open(system_prompt_filepath, 'r', encoding='utf-8') as sf:\n",
    "            system_prompt = sf.read()\n",
    "        with open(user_prompt_template_filepath, 'r', encoding='utf-8') as uf:\n",
    "            user_prompt = uf.read()\n",
    "        return system_prompt, user_prompt\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The prompt file was not found\")\n",
    "        return \"\"  # Return an empty string in case of an error"
   ],
   "id": "aa04cd91dbb0ba75",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. --- EVALUATION FUNCTION WITH STRUCTURED OUTPUT ---",
   "id": "850f8f3053ad599f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:41:34.082608Z",
     "start_time": "2025-10-29T15:41:34.075600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_extraction(document: str, extraction: str, system_prompt: str, user_prompt_template: str,\n",
    "                        pydantic_model: BaseModel) -> tuple:\n",
    "    \"\"\"\n",
    "    Prepares and calls the OpenAI API using 'response_model' for structured and validated output.\n",
    "    \"\"\"\n",
    "    if not system_prompt and user_prompt_template:\n",
    "        print(\"Cannot proceed without a prompt template.\")\n",
    "        return None, None\n",
    "\n",
    "    user_prompt_filled = user_prompt_template.format(\n",
    "        original_document=document,\n",
    "        extraction_to_evaluate=extraction\n",
    "    )\n",
    "\n",
    "    print(\"--- STARTING STRUCTURED EVALUATION ---\")\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            # messages=[{\"role\": \"user\", \"content\": filled_prompt}],\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt_filled}\n",
    "            ],  # 1. We define the \"tool\" that the model should use.\n",
    "            #    We use .model_json_schema() to automatically generate the schema.\n",
    "            tools=[\n",
    "                {\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": \"save_evaluation\",\n",
    "                        \"description\": \"Saves the structured evaluation result of an extraction.\",\n",
    "                        \"parameters\": pydantic_model.model_json_schema()\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            # 2. We force the model to call our tool.\n",
    "            tool_choice={\"type\": \"function\", \"function\": {\"name\": \"save_evaluation\"}}\n",
    "        )\n",
    "\n",
    "        # 3. We extract the result from the tool call arguments.\n",
    "        #    The response is a JSON string, not an object.\n",
    "        tool_call = response.choices[0].message.tool_calls[0]\n",
    "        json_arguments = tool_call.function.arguments\n",
    "\n",
    "        # We capture tokens' usage\n",
    "        token_usage = response.usage\n",
    "\n",
    "        # 4. We parse and validate the JSON against our Pydantic model.\n",
    "        evaluation_object = pydantic_model.model_validate_json(json_arguments)\n",
    "\n",
    "        return evaluation_object, token_usage\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred calling the API or processing the response: {e}\")\n",
    "        return None, None"
   ],
   "id": "a04d9c9294cfe927",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. --- MAIN ITERATION AND STORAGE LOGIC ---",
   "id": "ceec2ddf7ee558d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:41:34.101827Z",
     "start_time": "2025-10-29T15:41:34.091815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_dataset(\n",
    "        dataset: list,\n",
    "        prepare_tasks_func: callable,\n",
    "        system_prompt_path: str,\n",
    "        user_prompt_template_path: str,\n",
    "        pydantic_model: BaseModel,\n",
    "        env: str = \"development\",\n",
    "):\n",
    "    \"\"\"\n",
    "    A generic main loop to process any dataset using a specific task preparer.\n",
    "    \"\"\"\n",
    "    system_prompt, user_prompt_template = load_prompts_from_files(system_prompt_path, user_prompt_template_path)\n",
    "    if not system_prompt and user_prompt_template:\n",
    "        return [], 0\n",
    "\n",
    "    all_evaluations = []\n",
    "    expert_review_tasks = []\n",
    "    total_tokens = 0\n",
    "    docs_to_process = dataset[:1] if env == \"development\" else dataset\n",
    "\n",
    "    print(f\"Processing {len(docs_to_process)} documents...\")\n",
    "\n",
    "    for entry in docs_to_process:\n",
    "        for task in prepare_tasks_func(entry):\n",
    "            doc_id, original_text, extraction_to_evaluate, model_name = task\n",
    "\n",
    "            print(f\"\\n---> Evaluating '{model_name}' for doc: {doc_id}\")\n",
    "\n",
    "            evaluation_object, usage_data = evaluate_extraction(\n",
    "                original_text,\n",
    "                extraction_to_evaluate,\n",
    "                system_prompt,\n",
    "                user_prompt_template,\n",
    "                pydantic_model\n",
    "            )\n",
    "\n",
    "            if evaluation_object and usage_data:\n",
    "                total_tokens += usage_data.total_tokens\n",
    "                result_record = {\n",
    "                    \"document_idx\": doc_id,\n",
    "                    \"model_evaluated\": model_name,\n",
    "                    \"evaluation_data\": evaluation_object.model_dump(),\n",
    "                    \"token_usage\": {\n",
    "                        \"prompt_tokens\": usage_data.prompt_tokens,\n",
    "                        \"completion_tokens\": usage_data.completion_tokens,\n",
    "                        \"total_tokens\": usage_data.total_tokens\n",
    "                    },\n",
    "                }\n",
    "                all_evaluations.append(result_record)\n",
    "\n",
    "                # --- Create expert review structure using helper function ---\n",
    "                review_task = create_expert_review_task(\n",
    "                    doc_id,\n",
    "                    model_name,\n",
    "                    original_text,\n",
    "                    extraction_to_evaluate,\n",
    "                    evaluation_object\n",
    "                )\n",
    "                expert_review_tasks.append(review_task)\n",
    "\n",
    "                print(f\"  Result stored. Used tokens: {usage_data.total_tokens}\")\n",
    "\n",
    "    return all_evaluations, expert_review_tasks, total_tokens"
   ],
   "id": "f22c257f27f8d81f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6. --- EXECUTION BLOCK ---",
   "id": "ed2ff7fad5ba4221"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:43:54.314245Z",
     "start_time": "2025-10-29T15:41:34.112885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- Configure what you want to execute here ---\n",
    "    # Options: \"BASSE\" or \"FLARES\"\n",
    "    evaluation_dataset = os.environ.get(\"EVALUATION_DATASET\", \"BASSE\").upper()\n",
    "\n",
    "    # Get environment value\n",
    "    environment = os.getenv(\"ENVIRONMENT\", \"development\")\n",
    "\n",
    "    if not evaluation_dataset:\n",
    "        print(f\"Error: 'EVALUATION_DATASET' environment variable is not set.\")\n",
    "        exit()\n",
    "    if evaluation_dataset not in [\"BASSE\", \"FLARES\"]:\n",
    "        print(f\"Error: 'EVALUATION_DATASET' must be either 'BASSE' or 'FLARES'.\")\n",
    "        exit()\n",
    "\n",
    "    # --- EVALUATIONS PARAMETERS ---\n",
    "    BASSE_DATASET_FILEPATH = str(PROJECT_ROOT / 'data' / 'basse' / 'BASSE.jsonl')\n",
    "    FLARES_DATASET_FILEPATHS = [\n",
    "        str(PROJECT_ROOT / 'data' / 'flares' / '5w1h_subtarea_1_train.json'),\n",
    "        str(PROJECT_ROOT / 'data' / 'flares' / '5w1h_subtask_1_trial.json')\n",
    "    ]\n",
    "    SYSTEM_PROMPT_FILE = str(PROJECT_ROOT / 'prompts' / 'system_evaluation_prompt_v4.txt')\n",
    "    USER_PROMPT_FILE = str(PROJECT_ROOT / 'prompts' / 'user_evaluation_prompt_v4.txt')\n",
    "    # Make sure the Pydantic model to use is defined or imported\n",
    "    PYDANTIC_MODEL = DetailedEvaluation\n",
    "\n",
    "    dataset_to_run = None\n",
    "    task_preparer = None\n",
    "    output_filename = None\n",
    "\n",
    "    print(f\"Starting evaluation for target: {evaluation_dataset}\")\n",
    "\n",
    "    if evaluation_dataset == \"BASSE\":\n",
    "        dataset_to_run = load_basse_dataset(BASSE_DATASET_FILEPATH)\n",
    "        task_preparer = prepare_basse_tasks\n",
    "    elif evaluation_dataset == \"FLARES\":\n",
    "        dataset_to_run = load_flares_dataset(FLARES_DATASET_FILEPATHS)\n",
    "        task_preparer = prepare_flares_tasks\n",
    "\n",
    "    if dataset_to_run and task_preparer:\n",
    "        final_results, review_tasks, total_tokens = process_dataset(\n",
    "            dataset=dataset_to_run,\n",
    "            prepare_tasks_func=task_preparer,\n",
    "            system_prompt_path=SYSTEM_PROMPT_FILE,\n",
    "            user_prompt_template_path=USER_PROMPT_FILE,\n",
    "            pydantic_model=PYDANTIC_MODEL,\n",
    "            env=environment\n",
    "        )\n",
    "\n",
    "        # --- Create the dynamic filename ---\n",
    "\n",
    "        # 1. Get the current date\n",
    "        current_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "        # 2. Sanitize model name to be filename-friendly (replaces slashes)\n",
    "        safe_model_name = model_name.replace('/', '_')\n",
    "\n",
    "        # 3. Assemble the final filename\n",
    "        output_filename = f\"results/{current_date}_{environment}_{evaluation_dataset}_{ai_provider}_{safe_model_name}.json\"\n",
    "        output_review_task_filename = f\"results/{current_date}_{environment}_{evaluation_dataset}_{ai_provider}_{safe_model_name}_review.json\"\n",
    "\n",
    "        # --- Export the data and to results and review files ---\n",
    "\n",
    "        export_data = {\"total_tokens\": total_tokens, \"results\": final_results}\n",
    "\n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"\\nResults exported to: {output_filename}\")\n",
    "\n",
    "        with open(output_review_task_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(review_tasks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"\\nReview data exported to: {output_filename}\")\n",
    "    else:\n",
    "        print(\"Evaluation target not found or dataset could not be loaded.\")\n"
   ],
   "id": "73618fa8c57a32fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation for target: BASSE\n",
      "First object from the list of extractions:\n",
      "{\n",
      "  \"idx\": \"http://elpais.com/deportes/2019/08/17/actualidad/1566005143_044557.html\",\n",
      "  \"round\": 1,\n",
      "  \"original_document\": \"El jet lag ante Argentina , que quedó maquillado por el arrebato febril de Ricky ( 15 puntos en los últimos cuatro minutos ) , se consolidó 24 horas después ante Rusia . A tres días del comienzo del Mundial , de nuevo sin Marc Gasol en la rotación , el conjunto de Scariolo firmó su segunda derrota de la preparación y , por números y sensaciones , abrazó las dudas justo antes del estreno oficial . En el marcador , un contundente 55-74 ; en la estadística , unos pobres porcentajes de tiro ( 15 de 37 de dos , 3 de 18 en triples y 16 de 19 en tiros libres ) , 18 pérdidas , 12 robos del rival… Los 26 puntos y 11 rebotes de Willy Hernangómez fueron lo único lustroso de una prueba para olvidar , o para tentarse la ropa . “ Esperemos que esta no sea la referencia . Nos faltan muchos jugadores y la cabeza estará también en una dimensión diferente el sábado . La competición es otra cosa . Hemos jugado mal hay que olvidarlo pronto ” , analizó el seleccionador . Con las bajas de Rudy y Marc ( ambas por precaución ) , Scariolo reclutó a Llull en el quinteto inicial junto a Ricky , probó a Rabaseda apretando las tuercas en el puesto de tres y juntó a Claver con Willy Hernangómez en la pintura . Intentó ajustar automatismos la selección española , se aplicó de inicio en la faceta defensiva y se entregó en ataque a la pujanza de Willy y a la iluminación de Ricky . Pero todo duró un santiamén . Líder plenipotenciario sobre la pista durante todos los amistosos de la preparación mundialista , Ricky se ha convertido a un tiempo en metrónomo y artificiero del equipo y , en su partido 140 como internacional , tenía los minutos limitados por Scariolo : no más de 15 anunció el seleccionador en la previa . Así las cosas , y sin participación de su jugador franquicia en la segunda mitad , España navegó de la inconstancia al borrón , sin más penalización que las tribulaciones , de momento . Se fue imponiendo Rusia gracias a su dominio en el rebote y a la intendencia de Fridzon , Zubkov , Ivlev y Antonov . Y , como sucediera en el duelo ante Argentina , con el paso de los minutos , la espesura se adueñó del partido . Entre la prudencia y las probaturas , el ritmo anotador se fue resintiendo , sobre todo el español . En los ocho primeros minutos del segundo cuarto , los de Scariolo solo fueron capaces de sumar cuatro puntos desde el tiro libre . Aprovechó Rusia para armar un modesto estirón consolidado por una antideportiva a Ricky que permitió a los de Bazarevich alcanzar su máxima ventaja ( 25-32 , m. 18 ) . Justo ahí llegó el susto de la jornada , Ricky se lanzó a por un rebote y se lastimó , aparentemente sin consecuencias , el codo derecho . Una canasta de Willy , con fallo en el adicional , fue el único acierto en tiros de campo para España en su desastroso parcial ( 6-12 ) . Se mantuvo la inercia en la reanudación y Rusia alcanzó pronto el +10 en el marcador ( 27-37 , m. 22 ) . Ante el gesto serio y fiscalizador de Scariolo , Willy apretó los dientes y se multiplicó en defensa y en ataque para reenganchar a la selección española a la pelea . Nueve puntos del pívot de los Hornets en los cinco primeros minutos del cuarto capearon momentáneamente el temporal , pero fue tan solo un espejismo . Un robo de Motovilov a Llull estiró aún más la diferencia ( 36-47 , m. 27 ) . Fue el comienzo de la tormenta . El acierto de Rusia desde el perímetro y la falta de firmeza , acierto y concentración de España ( 16 pérdidas , 12 robos del rival , y un 1 de 13 en triples ) descosieron definitivamente el encuentro ( 40-59 , m. 30 ) . Con todo resuelto , Colom , Rabaseda , Beirán , Juancho y Oriola fue el quinteto asimétrico encargado de gestionar los minutos de la honrilla . Dos triples de Juancho rebajaron la proporción de la tunda ( 48-61 ) y Beirán tuvo el lanzamiento del rearme moral , pero se estrelló en el aro ruso . A golpe de orgullo , la diferencia bajó hasta los 8 ( 53-61 , m. 37 ) . Pero Bazarevich repasó la cartilla a los suyos para sacarles del embelesamiento , restablecer la distancia y barnizar de incertidumbre el último ensayo de España antes del estreno mundialista . Puedes seguir Deportes de EL PAÍS en Facebook , Twitter o suscribirte aquí a la Newsletter.\",\n",
      "  \"claude-5w1h_summ\": \"Aquí tienes un resumen del texto utilizando el método de las 5W1H:\\nQué: La selección española de baloncesto sufrió una derrota contundente (55-74) contra Rusia en un partido de preparación para el Mundial.\\nQuién: El equipo español dirigido por Sergio Scariolo, con actuaciones destacadas de Willy Hernangómez (26 puntos y 11 rebotes) y Ricky Rubio (limitado a 15 minutos de juego).\\nCuándo: A tres días del comienzo del Mundial de baloncesto.\\nDónde: No se especifica el lugar exacto del partido en el texto.\\nPor qué: El equipo español mostró un rendimiento deficiente, con bajos porcentajes de tiro, muchas pérdidas de balón y falta de concentración. Además, hubo ausencias importantes como las de Marc Gasol y Rudy Fernández.\\nCómo: España comenzó bien defensivamente, pero pronto perdió el control del juego. Rusia dominó en el rebote y aprovechó el mal juego español para aumentar su ventaja progresivamente. A pesar de un intento de remontada en el último cuarto, Rusia mantuvo su ventaja y se llevó la victoria.\\n¿Te gustaría que elaborara algún punto en particular de este resumen?\",\n",
      "  \"commandr-5w1h_summ\": \"Aquí está el resumen del texto utilizando el método 5W1H:\\n\\n\\nQué: Derrota de la selección española de baloncesto ante Rusia en un partido de preparación para el Mundial.\\n\\n\\nQuién: La selección española de baloncesto, liderada por Ricky y Willy Hernangómez.\\n\\n\\nCuándo: Tres días antes del comienzo del Mundial.\\n\\n\\nDónde: No se especifica el lugar, pero se entiende que fue en un partido amistoso de preparación para el Mundial.\\n\\n\\nPor qué: La selección española buscaba ajustar sus automatismos y prepararse para el Mundial, pero terminó sufriendo su segunda derrota en la preparación.\\n\\n\\nCómo: España tuvo problemas con el rebote, pérdidas de balón y bajos porcentajes de tiro, mientras que Rusia dominó el rebote y tuvo contribuciones de varios jugadores. El partido se fue complicando para España a medida que avanzaba, y a pesar de los esfuerzos de Willy Hernangómez, la diferencia en el marcador fue aumentando hasta llegar a una contundente derrota.\\n\\n\\n¿Qué? Derrota de España ante Rusia en un partido de preparación para el Mundial.\",\n",
      "  \"gpt4o-5w1h_summ\": \"Qué: La selección española de baloncesto sufrió su segunda derrota consecutiva en la preparación para el Mundial, esta vez contra Rusia, mostrando un rendimiento pobre en tiros y con muchas pérdidas de balón.\\n\\nQuién: El equipo dirigido por Sergio Scariolo, con actuaciones destacadas de Ricky Rubio y Willy Hernangómez.\\n\\nCuándo: A tres días del comienzo del Mundial, después de haber perdido previamente contra Argentina.\\n\\nDónde: El partido se disputó como parte de los amistosos de preparación para el Mundial.\\n\\nPor qué: España jugó sin algunos de sus jugadores clave como Marc Gasol y Rudy Fernández, lo que afectó su desempeño. Además, se vieron limitados por la ausencia de su jugador estrella, Ricky Rubio, en la segunda mitad.\\n\\nCómo: Rusia dominó el partido gracias a su control en el rebote y el juego colectivo. La selección española mostró debilidades defensivas y ofensivas, acumulando errores y bajos porcentajes de tiro.\",\n",
      "  \"reka-5w1h_summ\": \"Qué: España perdió su segundo partido de preparación para el Mundial de Baloncesto contra Rusia con un marcador de 55-74.\\n\\nQuién: El equipo español, dirigido por Sergio Scariolo, jugó sin Marc Gasol y sufrió las bajas de Rudy Fernández por precaución. Ricky Rubio lideró al equipo con 15 puntos en los últimos cuatro minutos del último encuentro previo al Mundial.\\n\\nCuándo: La derrota ocurrió 24 horas después del partido contra Argentina y tres días antes del inicio del Mundial.\\n\\nDónde: El partido se llevó a cabo en España, aunque no se especifica la ciudad exacta.\\n\\nPor qué: España mostró un bajo rendimiento en porcentajes de tiro (15 de 37 de dos, 3 de 18 en triples y 16 de 19 en tiros libres), cometió 18 pérdidas y permitió 12 robos a Rusia. La falta de varios jugadores clave también afectó el desempeño del equipo.\\n\\nCómo: Rusia se impuso gracias a su dominio en el rebote y la efectividad de jugadores como Fridzon, Zubkov, Ivlev y Antonov. A pesar de un esfuerzo de Willy Hernangómez con 26 puntos y 11 rebotes, España no logró remontar la diferencia.\",\n",
      "  \"llama3-5w1h_summ\": \"Aquí te presento el resumen del texto utilizando el método de las 5W1H (qué, quién, cuándo, dónde, por qué, cómo):\\n\\nQué: La selección española de baloncesto perdió ante Rusia en un partido de preparación para el Mundial, con un marcador de 55-74.\\n\\nQuién: La selección española, liderada por el entrenador Sergio Scariolo, y la selección rusa, liderada por el entrenador Sergei Bazarevich.\\n\\nCuándo: El partido se jugó tres días antes del comienzo del Mundial.\\n\\nDónde: No se especifica el lugar exacto del partido, pero se menciona que es un partido de preparación para el Mundial.\\n\\nPor qué: La selección española jugó mal y cometió muchos errores, lo que permitió a Rusia dominar el partido.\\n\\nCómo: La selección española jugó con una rotación reducida debido a las bajas de Rudy y Marc Gasol, y no pudo encontrar su ritmo anotador. Rusia, por su parte, dominó el rebote y contó con la intendencia de jugadores como Fridzon, Zubkov, Ivlev y Antonov. La selección española intentó reengancharse al partido gracias a los esfuerzos de Willy Hernangómez, pero no pudo evitar la derrota.\"\n",
      "}\n",
      "Processing 1 documents...\n",
      "\n",
      "---> Evaluating 'claude' for doc: http://elpais.com/deportes/2019/08/17/actualidad/1566005143_044557.html\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 4400\n",
      "\n",
      "---> Evaluating 'commandr' for doc: http://elpais.com/deportes/2019/08/17/actualidad/1566005143_044557.html\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 4542\n",
      "\n",
      "---> Evaluating 'gpt4o' for doc: http://elpais.com/deportes/2019/08/17/actualidad/1566005143_044557.html\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 4422\n",
      "\n",
      "---> Evaluating 'reka' for doc: http://elpais.com/deportes/2019/08/17/actualidad/1566005143_044557.html\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 5244\n",
      "\n",
      "---> Evaluating 'llama3' for doc: http://elpais.com/deportes/2019/08/17/actualidad/1566005143_044557.html\n",
      "--- STARTING STRUCTURED EVALUATION ---\n",
      "  Result stored. Used tokens: 4956\n",
      "\n",
      "Results exported to: results/2025-10-29_12-43-54_development_BASSE_openai_gpt-5-mini.json\n",
      "\n",
      "Review data exported to: results/2025-10-29_12-43-54_development_BASSE_openai_gpt-5-mini.json\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
