{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. --- IMPORTS AND CONFIGURATION ---",
   "id": "470bace0b6888015"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Importing the custom preprocessing function for BASSE dataset and preparation tasks\n",
    "from preprocessing.basse_preprocessing import process_basse_extractions\n",
    "from preparation.basse_preparation import prepare_basse_tasks\n",
    "\n",
    "# Importing the custom preprocessing functions for FLARES dataset and preparation tasks\n",
    "from preprocessing.flares_preprocessing import load_and_merge_datasets, process_and_flatten_data\n",
    "from preparation.flares_preparation import prepare_flares_tasks\n",
    "\n",
    "# Define the Pydantic models for structured output\n",
    "from pydantic_models.output_pydantic_models import DetailedEvaluation\n",
    "\n",
    "# Import the helper function to create the expert review task structure\n",
    "from validation.create_expert_review_task import create_expert_review_task\n",
    "\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "# Load environment variables (will look for .env file)\n",
    "load_dotenv()\n",
    "\n",
    "# --- Dynamic AI Provider Configuration ---\n",
    "\n",
    "# Read environment variables\n",
    "ai_provider = os.getenv(\"MODEL_PROVIDER\", \"openai\").lower()  # Default to 'openai'\n",
    "model_name = os.getenv(\"MODEL\", \"gpt-5-mini\") # Default to 'gpt-5-mini'\n",
    "\n",
    "api_key = None\n",
    "base_url = None\n",
    "\n",
    "print(f\"Using provider: {ai_provider.capitalize()}\")\n",
    "\n",
    "if ai_provider == \"openai\":\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    # base_url is not needed; the client defaults to OpenAI's endpoint.\n",
    "elif ai_provider == \"gemini\":\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "elif ai_provider == \"anthropic\":\n",
    "    api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "    base_url = \"https://api.anthropic.com/v1/\"\n",
    "elif ai_provider == \"openrouter\":\n",
    "    api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "    base_url = \"https://openrouter.ai/api/v1\"\n",
    "else:\n",
    "    print(f\"Error: AI provider '{ai_provider}' is not supported. Options: openai, gemini, anthropic.\")\n",
    "    exit()\n",
    "\n",
    "# --- Client Initialization ---\n",
    "\n",
    "# Validate that the API key and model are defined\n",
    "if not api_key:\n",
    "    print(f\"Error: API key environment variable for '{ai_provider}' is not set.\")\n",
    "    exit()\n",
    "if not model_name:\n",
    "    print(\"Error: 'MODEL' environment variable is not set.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize the OpenAI client with the dynamic configuration\n",
    "try:\n",
    "    client = openai.OpenAI(api_key=api_key, base_url=base_url)\n",
    "except openai.OpenAIError as e:\n",
    "    print(f\"Error initializing the client for {ai_provider.capitalize()}.\")\n",
    "    print(e)\n",
    "    exit()\n",
    "\n",
    "# Now you can use the client and model name in your API calls\n",
    "print(f\"âœ… Client initialized successfully. Model to use: {model_name}\")"
   ],
   "id": "9fb2a029de32d32b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. --- LOAD DATASETS ---",
   "id": "eebe3b8f481fa398"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_basse_dataset(basse_path: str, ):\n",
    "    \"\"\"\n",
    "    Loads the BASSE dataset from a JSONL file and processes it.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return process_basse_extractions(basse_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The BASSE dataset file was not found at path '{basse_path}'\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def load_flares_dataset(flares_path: list):\n",
    "    \"\"\"\n",
    "    Loads and processes the FLARES dataset from multiple JSON files.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load and merge the datasets\n",
    "        flares_datasets_merged = load_and_merge_datasets(flares_path)\n",
    "        # Process and flatten the data\n",
    "        return process_and_flatten_data(flares_datasets_merged)\n",
    "    except FileNotFoundError:\n",
    "        print(\n",
    "            f\"Error: The FLARES dataset files were not found at the specified paths: {flares_path[0]} and {flares_path[1]}\")\n",
    "        return []"
   ],
   "id": "50fa6f2717881f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. --- FUNCTION TO LOAD THE PROMPT ---",
   "id": "b9ba32a8ffff78dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_prompts_from_files(system_prompt_filepath: str, user_prompt_template_filepath) -> (str, str):\n",
    "    \"\"\"Reads and returns the content of a text file.\"\"\"\n",
    "    try:\n",
    "        with open(system_prompt_filepath, 'r', encoding='utf-8') as sf:\n",
    "            system_prompt = sf.read()\n",
    "        with open(user_prompt_template_filepath, 'r', encoding='utf-8') as uf:\n",
    "            user_prompt = uf.read()\n",
    "        return system_prompt, user_prompt\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The prompt file was not found\")\n",
    "        return \"\"  # Return an empty string in case of an error"
   ],
   "id": "aa04cd91dbb0ba75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. --- EVALUATION FUNCTION WITH STRUCTURED OUTPUT ---",
   "id": "850f8f3053ad599f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate_extraction(document: str, extraction: str, system_prompt: str, user_prompt_template: str,\n",
    "                        pydantic_model: BaseModel) -> tuple:\n",
    "    \"\"\"\n",
    "    Prepares and calls the OpenAI API using 'response_model' for structured and validated output.\n",
    "    \"\"\"\n",
    "    if not system_prompt and user_prompt_template:\n",
    "        print(\"Cannot proceed without a prompt template.\")\n",
    "        return None, None\n",
    "\n",
    "    user_prompt_filled = user_prompt_template.format(\n",
    "        original_document=document,\n",
    "        extraction_to_evaluate=extraction\n",
    "    )\n",
    "\n",
    "    print(\"--- STARTING STRUCTURED EVALUATION ---\")\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            # messages=[{\"role\": \"user\", \"content\": filled_prompt}],\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt_filled}\n",
    "            ],  # 1. We define the \"tool\" that the model should use.\n",
    "            #    We use .model_json_schema() to automatically generate the schema.\n",
    "            tools=[\n",
    "                {\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": \"save_evaluation\",\n",
    "                        \"description\": \"Saves the structured evaluation result of an extraction.\",\n",
    "                        \"parameters\": pydantic_model.model_json_schema()\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            # 2. We force the model to call our tool.\n",
    "            tool_choice={\"type\": \"function\", \"function\": {\"name\": \"save_evaluation\"}}\n",
    "        )\n",
    "\n",
    "        # 3. We extract the result from the tool call arguments.\n",
    "        #    The response is a JSON string, not an object.\n",
    "        tool_call = response.choices[0].message.tool_calls[0]\n",
    "        json_arguments = tool_call.function.arguments\n",
    "\n",
    "        # We capture tokens' usage\n",
    "        token_usage = response.usage\n",
    "\n",
    "        # 4. We parse and validate the JSON against our Pydantic model.\n",
    "        evaluation_object = pydantic_model.model_validate_json(json_arguments)\n",
    "\n",
    "        return evaluation_object, token_usage\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred calling the API or processing the response: {e}\")\n",
    "        return None, None"
   ],
   "id": "a04d9c9294cfe927",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. --- MAIN ITERATION AND STORAGE LOGIC ---",
   "id": "ceec2ddf7ee558d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def process_dataset(\n",
    "        dataset: list,\n",
    "        prepare_tasks_func: callable,\n",
    "        system_prompt_path: str,\n",
    "        user_prompt_template_path: str,\n",
    "        pydantic_model: BaseModel,\n",
    "        env: str = \"development\",\n",
    "):\n",
    "    \"\"\"\n",
    "    A generic main loop to process any dataset using a specific task preparer.\n",
    "    \"\"\"\n",
    "    system_prompt, user_prompt_template = load_prompts_from_files(system_prompt_path, user_prompt_template_path)\n",
    "    if not system_prompt and user_prompt_template:\n",
    "        return [], 0\n",
    "\n",
    "    all_evaluations = []\n",
    "    expert_review_tasks = []\n",
    "    total_tokens = 0\n",
    "    docs_to_process = dataset[:1] if env == \"development\" else dataset\n",
    "\n",
    "    print(f\"Processing {len(docs_to_process)} documents...\")\n",
    "\n",
    "    for entry in docs_to_process:\n",
    "        for task in prepare_tasks_func(entry):\n",
    "            doc_id, original_text, extraction_to_evaluate, model_name = task\n",
    "\n",
    "            print(f\"\\n---> Evaluating '{model_name}' for doc: {doc_id}\")\n",
    "\n",
    "            evaluation_object, usage_data = evaluate_extraction(\n",
    "                original_text,\n",
    "                extraction_to_evaluate,\n",
    "                system_prompt,\n",
    "                user_prompt_template,\n",
    "                pydantic_model\n",
    "            )\n",
    "\n",
    "            if evaluation_object and usage_data:\n",
    "                total_tokens += usage_data.total_tokens\n",
    "                result_record = {\n",
    "                    \"document_idx\": doc_id,\n",
    "                    \"model_evaluated\": model_name,\n",
    "                    \"evaluation_data\": evaluation_object.model_dump(),\n",
    "                    \"token_usage\": {\n",
    "                        \"prompt_tokens\": usage_data.prompt_tokens,\n",
    "                        \"completion_tokens\": usage_data.completion_tokens,\n",
    "                        \"total_tokens\": usage_data.total_tokens\n",
    "                    },\n",
    "                }\n",
    "                all_evaluations.append(result_record)\n",
    "\n",
    "                # --- Create expert review structure using helper function ---\n",
    "                review_task = create_expert_review_task(\n",
    "                    doc_id,\n",
    "                    model_name,\n",
    "                    original_text,\n",
    "                    extraction_to_evaluate,\n",
    "                    evaluation_object\n",
    "                )\n",
    "                expert_review_tasks.append(review_task)\n",
    "\n",
    "                print(f\"  Result stored. Used tokens: {usage_data.total_tokens}\")\n",
    "\n",
    "    return all_evaluations, expert_review_tasks, total_tokens"
   ],
   "id": "f22c257f27f8d81f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6. --- EXECUTION BLOCK ---",
   "id": "ed2ff7fad5ba4221"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- Configure what you want to execute here ---\n",
    "    # Options: \"BASSE\" or \"FLARES\"\n",
    "    evaluation_dataset = os.environ.get(\"EVALUATION_DATASET\", \"BASSE\").upper()\n",
    "\n",
    "    # Get environment value\n",
    "    environment = os.getenv(\"ENVIRONMENT\", \"development\")\n",
    "\n",
    "    if not evaluation_dataset:\n",
    "        print(f\"Error: 'EVALUATION_DATASET' environment variable is not set.\")\n",
    "        exit()\n",
    "    if evaluation_dataset not in [\"BASSE\", \"FLARES\"]:\n",
    "        print(f\"Error: 'EVALUATION_DATASET' must be either 'BASSE' or 'FLARES'.\")\n",
    "        exit()\n",
    "\n",
    "    # --- EVALUATIONS PARAMETERS ---\n",
    "    BASSE_DATASET_FILEPATH = str(PROJECT_ROOT / 'data' / 'basse' / 'BASSE.jsonl')\n",
    "    FLARES_DATASET_FILEPATHS = [\n",
    "        str(PROJECT_ROOT / 'data' / 'flares' / '5w1h_subtarea_1_train.json'),\n",
    "        str(PROJECT_ROOT / 'data' / 'flares' / '5w1h_subtask_1_trial.json')\n",
    "    ]\n",
    "    SYSTEM_PROMPT_FILE = str(PROJECT_ROOT / 'prompts' / 'system_evaluation_prompt.txt')\n",
    "    USER_PROMPT_FILE = str(PROJECT_ROOT / 'prompts' / 'user_evaluation_prompt.txt')\n",
    "    # Make sure the Pydantic model to use is defined or imported\n",
    "    PYDANTIC_MODEL = DetailedEvaluation\n",
    "\n",
    "    dataset_to_run = None\n",
    "    task_preparer = None\n",
    "    output_filename = None\n",
    "\n",
    "    print(f\"Starting evaluation for target: {evaluation_dataset}\")\n",
    "\n",
    "    if evaluation_dataset == \"BASSE\":\n",
    "        dataset_to_run = load_basse_dataset(BASSE_DATASET_FILEPATH)\n",
    "        task_preparer = prepare_basse_tasks\n",
    "    elif evaluation_dataset == \"FLARES\":\n",
    "        dataset_to_run = load_flares_dataset(FLARES_DATASET_FILEPATHS)\n",
    "        task_preparer = prepare_flares_tasks\n",
    "\n",
    "    if dataset_to_run and task_preparer:\n",
    "        final_results, review_tasks, total_tokens = process_dataset(\n",
    "            dataset=dataset_to_run,\n",
    "            prepare_tasks_func=task_preparer,\n",
    "            system_prompt_path=SYSTEM_PROMPT_FILE,\n",
    "            user_prompt_template_path=USER_PROMPT_FILE,\n",
    "            pydantic_model=PYDANTIC_MODEL,\n",
    "            env=environment\n",
    "        )\n",
    "\n",
    "        # --- Create the dynamic filename ---\n",
    "\n",
    "        # 1. Get the current date\n",
    "        current_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "        # 2. Sanitize model name to be filename-friendly (replaces slashes)\n",
    "        safe_model_name = model_name.replace('/', '_')\n",
    "\n",
    "        # 3. Assemble the final filename\n",
    "        output_filename = f\"results/{current_date}_{environment}_{evaluation_dataset}_{ai_provider}_{safe_model_name}.json\"\n",
    "        output_review_task_filename = f\"results/{current_date}_{environment}_{evaluation_dataset}_{ai_provider}_{safe_model_name}_review.json\"\n",
    "\n",
    "        # --- Export the data and to results and review files ---\n",
    "\n",
    "        export_data = {\"total_tokens\": total_tokens, \"results\": final_results}\n",
    "\n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"\\nResults exported to: {output_filename}\")\n",
    "\n",
    "        with open(output_review_task_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(review_tasks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"\\nReview data exported to: {output_filename}\")\n",
    "    else:\n",
    "        print(\"Evaluation target not found or dataset could not be loaded.\")\n"
   ],
   "id": "73618fa8c57a32fa",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
